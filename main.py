import yaml
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import os,argparse
from data_utils.nyuv2_dataset import NYUv2Dataset
import h5py,logging
# --- å¿…æ”¹1: ç¡®è®¤æ¨¡å—/æ–‡ä»¶åä¸€è‡´æ€§ ---
# è¯·ç¡®ä¿ä¸‹é¢çš„å¯¼å…¥è·¯å¾„ä¸æ‚¨é¡¹ç›®ä¸­models/å’Œlosses/ä¸‹çš„æ–‡ä»¶åå®Œå…¨ä¸€è‡´
# ä¾‹å¦‚ï¼Œå¦‚æœæ–‡ä»¶åæ˜¯ causal_models.py (å¤æ•°)ï¼Œåˆ™åº”æ”¹ä¸º:
# from models.causal_models import CausalMTLModel
from models.causal_model import CausalMTLModel
from losses.composite_loss import CompositeLoss,AdaptiveCompositeLoss
from datetime import datetime
from engine.trainer import train
from engine.visualizer import generate_visual_reports
from utils.general_utils import set_seed,setup_logging
from torch.utils.data import Subset

def main(config_path):
    """
    é¡¹ç›®ä¸»å‡½æ•°ï¼ˆæœ€ç»ˆé²æ£’ç‰ˆï¼‰ï¼Œé›†æˆäº†æ‰€æœ‰ç¨³å®šæ€§ä¸å¯å¤ç°æ€§ä¿®æ­£ã€‚
    """
    # 1. åŠ è½½é…ç½®å¹¶è®¾ç½®éšæœºç§å­
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        set_seed(config['training']['seed'])
    except Exception as e:
        logging.info(f"âŒ Error loading config file: {e}")
        return

    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')
    run_dir = os.path.join('runs', timestamp)
    checkpoint_dir = os.path.join(run_dir, 'checkpoints')
    vis_dir = os.path.join(run_dir, 'visualizations')
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(vis_dir, exist_ok=True)
    setup_logging(run_dir)
    logging.info("âœ… Configuration loaded successfully.")
    logging.info("ğŸ§© Full configuration:\n" + yaml.dump(config, sort_keys=False, allow_unicode=True))
    logging.info(f"ğŸŒ± Random seed set to {config['training']['seed']}")
    logging.info(f"ğŸ“‚ All outputs for this run will be saved in: {run_dir}")
    # 2. è®¾ç½®è®¡ç®—è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"ğŸš€ Using device: {device}")

    # 3. åˆå§‹åŒ–æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    logging.info("\nInitializing dataset...")
    try:
        # ç¡®ä¿æˆ‘ä»¬å¯¼å…¥çš„Datasetç±»åä¸æ–‡ä»¶åä¸­çš„ç±»åä¸€è‡´

        logging.info("Pre-loading scene metadata from HDF5 file...")
        with h5py.File(config['data']['dataset_path'], 'r') as db:
            scene_type_refs = db['sceneTypes']  # shape is (1, 1449)
            scene_types_list = []

            for i in range(scene_type_refs.shape[1]):
                ref = scene_type_refs[0, i]
                scene_str = "".join(chr(c[0]) for c in db[ref])
                scene_types_list.append(scene_str)
        full_dataset = NYUv2Dataset(
            mat_file_path=config['data']['dataset_path'],
            img_size=tuple(config['data']['img_size']),scene_types_list=scene_types_list
        )

        # --- å¿…æ”¹7: ä¿è¯éšæœºåˆ’åˆ†çš„å¯å¤ç°æ€§ ---
        g = torch.Generator()
        g.manual_seed(config['training']['seed'])
        train_size = int(0.8 * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=g)

        # --- å¿…æ”¹4: æ ¹æ®è®¾å¤‡æƒ…å†µè®¾ç½®pin_memory ---
        pin_memory = config['data'].get('pin_memory', torch.cuda.is_available())
        logging.info(f"ğŸ’¡ pin_memory set to: {pin_memory}")

        train_loader = DataLoader(
            train_dataset, batch_size=config['data']['batch_size'], shuffle=True,
            num_workers=config['data']['num_workers'], pin_memory=pin_memory
        )
        val_loader = DataLoader(
            val_dataset, batch_size=config['data']['batch_size'], shuffle=False,
            num_workers=config['data']['num_workers'], pin_memory=pin_memory
        )
        logging.info(f"ğŸ“š Dataset split into {len(train_dataset)} training and {len(val_dataset)} validation samples.")
    except Exception as e:
        logging.info(f"âŒ Error creating dataset/loaders: {e}")
        return

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’ŒæŸå¤±å‡½æ•°
    logging.info("\nInitializing model and training components...")
    model = CausalMTLModel(
        model_config=config['model'],
        data_config=config['data']
    ).to(device)
    base_lr = float(config['training']['learning_rate'])  # ä¾‹å¦‚ 1e-5

    # 2. åˆ†ç¦»å‚æ•°
    # è·å– encoder çš„å‚æ•°å†…å­˜åœ°å€ ID
    encoder_params_ids = list(map(id, model.encoder.parameters()))

    # è¿‡æ»¤å‚æ•°ï¼šä¸åœ¨ encoder ä¸­çš„å°±æ˜¯ head/decoder å‚æ•°
    backbone_params = model.encoder.parameters()
    head_params = [p for n, p in model.named_parameters() if id(p) not in encoder_params_ids]

    print(f"ğŸ”§ Optimizer setup: Backbone LR={base_lr}, Head/Decoder LR={base_lr * 10.0}")

    if config['training']['optimizer'] == 'AdamW':
        optimizer = optim.AdamW([
            {'params': backbone_params, 'lr': base_lr},  # é¢„è®­ç»ƒéƒ¨åˆ†ä¿æŒå° LR
            {'params': head_params, 'lr': base_lr * 10.0}  # æ–°å¢éƒ¨åˆ†æ”¾å¤§ 10 å€ LR
        ], weight_decay=config['training']['weight_decay'])
    else:
        optimizer = optim.Adam([
            {'params': backbone_params, 'lr': base_lr},
            {'params': head_params, 'lr': base_lr * 10.0}
        ])

    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    criterion = AdaptiveCompositeLoss(loss_weights=config['losses']).to(device)
    logging.info("âš™ï¸ Model, optimizer, scheduler, and loss function are ready.")
    # --- å¿…æ”¹6: éœ€ç¡®è®¤CompositeLossçš„è¿”å›æ¥å£ä¸trainerå…¼å®¹ ---
    # æˆ‘ä»¬å·²åœ¨ä¸Šä¸€ç‰ˆä¸­ç»Ÿä¸€ CompositeLoss è¿”å› (total_loss, loss_dict)ï¼Œ
    # å¹¶ä¸” trainer.py ä¸­çš„ä»£ç å·²å…¼å®¹æ­¤æ ¼å¼ã€‚

    # 6. å¯åŠ¨è®­ç»ƒæµç¨‹
    logging.info("\n----- Starting Training -----")
    if config['training'].get('enable_training', True):
        train(model, train_loader, val_loader, optimizer, criterion, scheduler, config, device, checkpoint_dir)
    else:
        logging.info("ğŸƒ Training is disabled in the config. Skipping.")
    from engine.experiments import run_all_experiments
    exp_cfg = config.get('experiments', {})
    if exp_cfg.get('enable', False):
        logging.info("\n===== Running falsification experiments =====")
        model.eval()
        _ = run_all_experiments(
            model, val_loader, device,
            max_batches_swap=int(exp_cfg.get('max_batches_swap', 8)),
            max_batches_inv=int(exp_cfg.get('max_batches_inv', 8)),
            max_batches_cross=int(exp_cfg.get('max_batches_cross', 20)),
        )

    # 7. æœ€ç»ˆå¯è§†åŒ–ä¸åˆ†æ
    logging.info("\n----- Running Final Visualizations & Analysis -----")
    best_checkpoint_path = os.path.join(checkpoint_dir, 'model_best.pth.tar')
    if os.path.exists(best_checkpoint_path):
        logging.info(f"ğŸ” Loading best model from {best_checkpoint_path} for visualization...")
        checkpoint = torch.load(best_checkpoint_path, map_location=device)
        try:
            model.load_state_dict(checkpoint['state_dict'])
            logging.info("âœ… Loaded checkpoint state_dict successfully.")
        except RuntimeError as e:
            logging.info(f"âš ï¸ Warning: state_dict load error: {e}. Trying non-strict load.")
            model.load_state_dict(checkpoint['state_dict'], strict=False)
        model.eval()
        vis_loader = DataLoader(val_dataset, batch_size=2, shuffle=True)
        # --- ä¿®æ”¹: å°†æ–°åˆ›å»ºçš„ vis_dir ä¼ é€’ç»™å¯è§†åŒ–å‡½æ•° ---
        generate_visual_reports(model, vis_loader, device, save_dir=vis_dir,num_reports=5)
    else:
        logging.info(f"âš ï¸ Could not find best model checkpoint at '{best_checkpoint_path}'. Skipping final analysis.")
    # --- å¿…æ”¹3: å®‰å…¨åœ°è°ƒç”¨closeæ–¹æ³• ---
    if hasattr(full_dataset, "close") and callable(full_dataset.close):
        logging.info("Closing dataset handler...")
        full_dataset.close()

    logging.info("\nğŸ‰ Project execution finished.")



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Causal MTL Training")
    # æ·»åŠ  --config å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰æä¾›ï¼Œé»˜è®¤ä½¿ç”¨ base_full_model.yaml
    parser.add_argument('--config', type=str, default='configs/base_full_model.yaml', help='Path to the config file')

    args = parser.parse_args()

    # ä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°
    config_file = args.config

    print(f"ğŸš€ Loading configuration from: {config_file}")  # æ‰“å°ä¸€ä¸‹ä»¥ç¡®è®¤

    # å¼ºçƒˆå»ºè®®: åœ¨æ­£å¼è¿è¡Œå‰ï¼Œç”¨ä¸€å°éƒ¨åˆ†æ•°æ®è¿›è¡Œå†’çƒŸæµ‹è¯•(smoke test)
    main(config_file)