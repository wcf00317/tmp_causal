import yaml
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import os,argparse
from data_utils.nyuv2_dataset import NYUv2Dataset
from data_utils.gta5_dataset import GTA5Dataset
import h5py,logging
from models.causal_model import CausalMTLModel
from losses.composite_loss import AdaptiveCompositeLoss
from losses.mtl_loss import MTLLoss
from models.baselines import RawMTLModel, SingleTaskModel
from losses.single_task_loss import SingleTaskLoss
from data_utils.cityscapes_dataset import CityscapesDataset
from datetime import datetime
from engine.trainer import train
from engine.visualizer import generate_visual_reports
from utils.general_utils import set_seed,setup_logging
from torch.utils.data import Subset

def main(config_path):
    """
    é¡¹ç›®ä¸»å‡½æ•°ï¼ˆæœ€ç»ˆé²æ£’ç‰ˆï¼‰ï¼Œé›†æˆäº†æ‰€æœ‰ç¨³å®šæ€§ä¸å¯å¤ç°æ€§ä¿®æ­£ã€‚
    """
    # 1. åŠ è½½é…ç½®å¹¶è®¾ç½®éšæœºç§å­
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        set_seed(config['training']['seed'])
    except Exception as e:
        logging.info(f"âŒ Error loading config file: {e}")
        return

    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')
    run_dir = os.path.join('runs', timestamp)
    checkpoint_dir = os.path.join(run_dir, 'checkpoints')
    vis_dir = os.path.join(run_dir, 'visualizations')
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(vis_dir, exist_ok=True)
    setup_logging(run_dir)
    logging.info("âœ… Configuration loaded successfully.")
    logging.info("ğŸ§© Full configuration:\n" + yaml.dump(config, sort_keys=False, allow_unicode=True))
    logging.info(f"ğŸŒ± Random seed set to {config['training']['seed']}")
    logging.info(f"ğŸ“‚ All outputs for this run will be saved in: {run_dir}")
    # 2. è®¾ç½®è®¡ç®—è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"ğŸš€ Using device: {device}")

    # 3. åˆå§‹åŒ–æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    logging.info("\nInitializing dataset...")
    try:
        data_cfg = config['data']
        dataset_type = data_cfg.get('type', 'nyuv2').lower()  # è·å–ç±»å‹ï¼Œé»˜è®¤ä¸º 'nyuv2' ä»¥å…¼å®¹æ—§é…ç½®
        #dataset_path = data_cfg['dataset_path']
        img_size = tuple(data_cfg['img_size'])

        logging.info(f"ğŸ“‹ Dataset Type Configuration: {dataset_type}")
        if dataset_type == 'gta5_to_cityscapes':
            logging.info("ğŸŒ Mode: Sim-to-Real (Train on GTA5, Val on Cityscapes)")
            # 1. åŠ è½½ GTA5 Train
            train_path = data_cfg['train_dataset_path']
            logging.info(f"   -> Loading Source Train: {train_path}")
            train_dataset = GTA5Dataset(root_dir=train_path, img_size=img_size)

            # 2. åŠ è½½ Cityscapes Val (Target)
            target_val_path = data_cfg['val_dataset_path']
            logging.info(f"   -> Loading Target Val: {target_val_path}")
            val_dataset = CityscapesDataset(root_dir=target_val_path, split='val', img_size=img_size)

            # 3. åŠ è½½ GTA5 Val (Source Held-out)
            source_val_path = data_cfg.get('source_val_path')
            if source_val_path and os.path.exists(source_val_path):
                logging.info(f"   -> Loading Source Val: {source_val_path}")
                source_val_dataset = GTA5Dataset(root_dir=source_val_path, img_size=img_size)
            else:
                logging.warning(f"âš ï¸ Source val path not found or empty: {source_val_path}")

            # è¿™é‡Œçš„ full_dataset åªæ˜¯ä¸ºäº†å…¼å®¹åé¢çš„ä¸€è¡Œä»£ç ï¼Œå¯ä»¥æŒ‡å‘ train_dataset
            full_dataset = train_dataset
        # === æ˜¾å¼åˆ†æ”¯é€»è¾‘ ===
        elif dataset_type == 'cityscapes':
            dataset_path = data_cfg['dataset_path']
            logging.info(f"ğŸ“‚ Loading CityscapesDataset from: {dataset_path}")
            full_dataset = CityscapesDataset(
                root_dir=dataset_path,
                split='train',
                img_size=img_size
            )

        elif dataset_type == 'nyuv2':
            dataset_path = data_cfg['dataset_path']
            logging.info(f"ğŸ“„ Loading NYUv2Dataset (HDF5) from: {dataset_path}")

            # NYUv2 ç‰¹æœ‰çš„é¢„è¯»å–é€»è¾‘
            logging.info("Pre-loading scene metadata from HDF5 file...")
            with h5py.File(dataset_path, 'r') as db:
                scene_type_refs = db['sceneTypes']
                scene_types_list = []
                for i in range(scene_type_refs.shape[1]):
                    ref = scene_type_refs[0, i]
                    scene_str = "".join(chr(c[0]) for c in db[ref])
                    scene_types_list.append(scene_str)

            full_dataset = NYUv2Dataset(
                mat_file_path=dataset_path,
                img_size=img_size,
                scene_types_list=scene_types_list
            )

        else:
            # é‡åˆ°ä¸æ”¯æŒçš„ç±»å‹ç›´æ¥æŠ¥é”™ï¼Œè€Œä¸æ˜¯ççŒœ
            raise ValueError(f"âŒ Unsupported dataset type: '{dataset_type}'. "
                             f"Supported types are: ['cityscapes', 'nyuv2']")


        g = torch.Generator()
        g.manual_seed(config['training']['seed'])
        train_size = int(0.8 * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=g)

        # --- å¿…æ”¹4: æ ¹æ®è®¾å¤‡æƒ…å†µè®¾ç½®pin_memory ---
        pin_memory = config['data'].get('pin_memory', torch.cuda.is_available())
        logging.info(f"ğŸ’¡ pin_memory set to: {pin_memory}")

        train_loader = DataLoader(
            train_dataset, batch_size=config['data']['batch_size'], shuffle=True,
            num_workers=config['data']['num_workers'], pin_memory=pin_memory
        )
        val_loader = DataLoader(
            val_dataset, batch_size=config['data']['batch_size'], shuffle=False,
            num_workers=config['data']['num_workers'], pin_memory=pin_memory
        )
        logging.info(f"ğŸ“š Dataset split into {len(train_dataset)} training and {len(val_dataset)} validation samples.")
    except Exception as e:
        logging.info(f"âŒ Error creating dataset/loaders: {e}")
        return

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’ŒæŸå¤±å‡½æ•°
    logging.info("\nInitializing model and training components...")
    model_type = config['model'].get('type', 'causal')
    base_lr = float(config['training']['learning_rate'])
    if model_type == 'raw_mtl':
        logging.info("ğŸ—ï¸ Building Baseline: Raw MTL Model")
        model = RawMTLModel(
            model_config=config['model'],
            data_config=config['data']
        ).to(device)

        # Baseline ä½¿ç”¨é€šç”¨ Loss
        strategy = config['training'].get('strategy', 'fixed')
        use_uncertainty = (strategy == 'uncertainty')
        logging.info(f" Using Loss Strategy: {strategy}")
        criterion = MTLLoss(loss_weights=config['losses'], use_uncertainty=use_uncertainty).to(device)
        optimizer = optim.AdamW([
            {'params': model.encoder.parameters(), 'lr': base_lr},
            {'params': model.seg_head.parameters(), 'lr': base_lr * 10},
            {'params': model.depth_head.parameters(), 'lr': base_lr * 10},
            {'params': model.scene_mlp.parameters(), 'lr': base_lr * 10},
            {'params': model.shared_proj.parameters(), 'lr': base_lr * 10},
            # å¦‚æœä½¿ç”¨ uncertaintyï¼Œloss ä¸­ä¹Ÿæœ‰å‚æ•°éœ€è¦ä¼˜åŒ–
            {'params': criterion.parameters(), 'lr': base_lr}
        ], weight_decay=config['training']['weight_decay'])
        scheduler = None
    elif model_type == 'single_task':
        logging.info(f"Building Single-Task Baseline: {config['model']['active_task']}")
        model = SingleTaskModel(
            model_config=config['model'],
            data_config=config['data']
        ).to(device)

        criterion = SingleTaskLoss(
            active_task=config['model']['active_task'],
            loss_weights=config['losses']
        ).to(device)

        # åªå°† encoder, shared_proj å’Œå½“å‰ä»»åŠ¡çš„ head æ”¾å…¥ä¼˜åŒ–å™¨
        # æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç”¨ model.parameters()ï¼Œå› ä¸ºå…¶ä»– head æ ¹æœ¬æ²¡æœ‰è¢«å®ä¾‹åŒ–
        optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=config['training']['weight_decay'])
        scheduler = None
    else:
        logging.info("Building Our Causal MTL Model")
        model = CausalMTLModel(
            model_config=config['model'],
            data_config=config['data']
        ).to(device)

        # 2. åˆ†ç¦»å‚æ•°
        # è·å– encoder çš„å‚æ•°å†…å­˜åœ°å€ ID
        encoder_params_ids = list(map(id, model.encoder.parameters()))

        # è¿‡æ»¤å‚æ•°ï¼šä¸åœ¨ encoder ä¸­çš„å°±æ˜¯ head/decoder å‚æ•°
        backbone_params = model.encoder.parameters()
        head_params = [p for n, p in model.named_parameters() if id(p) not in encoder_params_ids]

        print(f"ğŸ”§ Optimizer setup: Backbone LR={base_lr}, Head/Decoder LR={base_lr * 10.0}")

        if config['training']['optimizer'] == 'AdamW':
            optimizer = optim.AdamW([
                {'params': backbone_params, 'lr': base_lr},  # é¢„è®­ç»ƒéƒ¨åˆ†ä¿æŒå° LR
                {'params': head_params, 'lr': base_lr * 10.0}  # æ–°å¢éƒ¨åˆ†æ”¾å¤§ 10 å€ LR
            ], weight_decay=config['training']['weight_decay'])
        else:
            optimizer = optim.Adam([
                {'params': backbone_params, 'lr': base_lr},
                {'params': head_params, 'lr': base_lr * 10.0}
            ])

        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        criterion = AdaptiveCompositeLoss(loss_weights=config['losses']).to(device)
        logging.info("âš™ï¸ Model, optimizer, scheduler, and loss function are ready.")

    # 6. å¯åŠ¨è®­ç»ƒæµç¨‹
    logging.info("\n----- Starting Training -----")
    if config['training'].get('enable_training', True):
        train(model, train_loader, val_loader, optimizer, criterion, scheduler, config, device, checkpoint_dir)
    else:
        logging.info("ğŸƒ Training is disabled in the config. Skipping.")
    from engine.experiments import run_all_experiments
    exp_cfg = config.get('experiments', {})
    if exp_cfg.get('enable', False):
        logging.info("\n===== Running falsification experiments =====")
        model.eval()
        _ = run_all_experiments(
            model, val_loader, device,
            max_batches_swap=int(exp_cfg.get('max_batches_swap', 8)),
            max_batches_inv=int(exp_cfg.get('max_batches_inv', 8)),
            max_batches_cross=int(exp_cfg.get('max_batches_cross', 20)),
        )

    # 7. æœ€ç»ˆå¯è§†åŒ–ä¸åˆ†æ
    logging.info("\n----- Running Final Visualizations & Analysis -----")
    best_checkpoint_path = os.path.join(checkpoint_dir, 'model_best.pth.tar')
    if os.path.exists(best_checkpoint_path):
        logging.info(f"ğŸ” Loading best model from {best_checkpoint_path} for visualization...")
        checkpoint = torch.load(best_checkpoint_path, map_location=device)
        try:
            model.load_state_dict(checkpoint['state_dict'])
            logging.info("âœ… Loaded checkpoint state_dict successfully.")
        except RuntimeError as e:
            logging.info(f"âš ï¸ Warning: state_dict load error: {e}. Trying non-strict load.")
            model.load_state_dict(checkpoint['state_dict'], strict=False)
        model.eval()
        vis_loader = DataLoader(val_dataset, batch_size=2, shuffle=True)
        # --- ä¿®æ”¹: å°†æ–°åˆ›å»ºçš„ vis_dir ä¼ é€’ç»™å¯è§†åŒ–å‡½æ•° ---
        generate_visual_reports(model, vis_loader, device, save_dir=vis_dir,num_reports=5)
    else:
        logging.info(f"âš ï¸ Could not find best model checkpoint at '{best_checkpoint_path}'. Skipping final analysis.")
    if hasattr(full_dataset, "close") and callable(full_dataset.close):
        logging.info("Closing dataset handler...")
        full_dataset.close()

    logging.info("\nğŸ‰ Project execution finished.")



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Causal MTL Training")
    # æ·»åŠ  --config å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰æä¾›ï¼Œé»˜è®¤ä½¿ç”¨ base_full_model.yaml
    parser.add_argument('--config', type=str, default='configs/base_full_model.yaml', help='Path to the config file')

    args = parser.parse_args()

    # ä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°
    config_file = args.config

    print(f"ğŸš€ Loading configuration from: {config_file}")  # æ‰“å°ä¸€ä¸‹ä»¥ç¡®è®¤

    # å¼ºçƒˆå»ºè®®: åœ¨æ­£å¼è¿è¡Œå‰ï¼Œç”¨ä¸€å°éƒ¨åˆ†æ•°æ®è¿›è¡Œå†’çƒŸæµ‹è¯•(smoke test)
    main(config_file)