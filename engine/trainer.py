import torch
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR
from tqdm import tqdm
import os, logging
import numpy as np  # å¿…éœ€å¯¼å…¥
from .evaluator import evaluate
from utils.general_utils import save_checkpoint
from torch.cuda.amp import autocast, GradScaler


# ----------------------------
# utils
# ----------------------------
def _set_requires_grad(module, requires_grad: bool):
    if module is None:
        return
    for p in module.parameters():
        p.requires_grad = requires_grad


def _switch_stage_freeze(model, stage: int):
    """
    stage=1: å†»ç»“ z_p ç›¸å…³åˆ†æ”¯ï¼ˆç§æœ‰æŠ•å½±/æ®‹å·®ï¼‰ï¼Œåªè®­ç»ƒ z_s ä¸ä¸»å¹²ã€‚
    stage=2: å…¨éƒ¨è§£å†»ã€‚
    """
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰è¿™äº›å±æ€§ï¼ˆå…¼å®¹ RawMTLï¼‰
    if not hasattr(model, 'projector_p_seg') or model.projector_p_seg is None:
        return

    def _switch_stage_freeze(model, stage: int):
        # åŸºç¡€æ£€æŸ¥ï¼Œé˜²æ­¢ä¼ å…¥ä¸å…¼å®¹çš„æ¨¡å‹
        if not hasattr(model, 'projector_p_seg') or model.projector_p_seg is None:
            return

        # === Stage 0: åˆ†è§£é¢„çƒ­ (Decomposition Warmup) ===
        if stage == 0:
            # 1. å†»ç»“ä¸‹æ¸¸ä»»åŠ¡å¤´ (Seg, Depth, Normal)
            #    Stage 0 åªæƒ³è®­ç»ƒ Encoder å’Œ åˆ†è§£å¤´(Albedo/Normal/Light)ï¼Œ
            #    é˜²æ­¢éšæœºåˆå§‹åŒ–çš„ä»»åŠ¡å¤´å›ä¼ å¹²æ‰°æ¢¯åº¦ã€‚
            _set_requires_grad(model.predictor_seg, False)
            _set_requires_grad(model.predictor_depth, False)
            # ä½¿ç”¨ getattr å…¼å®¹å¯èƒ½æ²¡æœ‰ Normal ä»»åŠ¡çš„æ—§ Configï¼Œä½†æ‚¨ç¡®è®¤æœ‰ Normal
            if hasattr(model, 'predictor_normal'):
                _set_requires_grad(model.predictor_normal, False)

            # 2. å†»ç»“ z_p ç§æœ‰åˆ†æ”¯ (åŒ Stage 1)
            #    Stage 0 ä¸“æ³¨äº z_s çš„å‡ ä½•ç»“æ„ï¼ˆé€šè¿‡ Normal Head ç›‘ç£ï¼‰
            _set_requires_grad(model.projector_p_seg, False)
            _set_requires_grad(model.projector_p_depth, False)
            _set_requires_grad(getattr(model, 'projector_p_normal', None), False)

            _set_requires_grad(model.proj_z_p_seg, False)
            _set_requires_grad(model.proj_z_p_depth, False)
            _set_requires_grad(getattr(model, 'proj_z_p_normal', None), False)

            _set_requires_grad(model.zp_seg_refiner, False)
            _set_requires_grad(model.zp_depth_refiner, False)
            _set_requires_grad(getattr(model, 'zp_normal_refiner', None), False)

            _set_requires_grad(model.decoder_zp_depth, False)
            _set_requires_grad(getattr(model, 'decoder_zp_normal', None), False)

            logging.info("Stage-0: Decomposition Warmup. Frozen Task Heads & z_p branches.")

        # === Stage 1: ç»“æ„é¢„çƒ­ (Structure Warmup) ===
        elif stage == 1:
            # 1. [å…³é”®ä¿®æ”¹] æ˜¾å¼è§£å†»ä»»åŠ¡å¤´
            #    å› ä¸ºå®ƒä»¬åœ¨ Stage 0 è¢«å†»ç»“äº†ï¼Œå¿…é¡»åœ¨è¿™é‡Œè§£å¼€ï¼
            _set_requires_grad(model.predictor_seg, True)
            _set_requires_grad(model.predictor_depth, True)
            if hasattr(model, 'predictor_normal'):
                _set_requires_grad(model.predictor_normal, True)

            # 2. ç»§ç»­å†»ç»“ z_p ç§æœ‰åˆ†æ”¯ (ä¿æŒåŸæ ·)
            _set_requires_grad(model.projector_p_seg, False)
            _set_requires_grad(model.projector_p_depth, False)
            _set_requires_grad(getattr(model, 'projector_p_normal', None), False)

            _set_requires_grad(model.proj_z_p_seg, False)
            _set_requires_grad(model.proj_z_p_depth, False)
            _set_requires_grad(getattr(model, 'proj_z_p_normal', None), False)

            _set_requires_grad(model.zp_seg_refiner, False)
            _set_requires_grad(model.zp_depth_refiner, False)
            _set_requires_grad(getattr(model, 'zp_normal_refiner', None), False)

            _set_requires_grad(model.decoder_zp_depth, False)
            _set_requires_grad(getattr(model, 'decoder_zp_normal', None), False)

            logging.info("Stage-1: Structure Warmup. Frozen z_p branches, Unfrozen Task Heads.")

        # === Stage 2: å…¨é¢è®­ç»ƒ (Full Training) ===
        else:
            # 1. ç¡®ä¿ä»»åŠ¡å¤´æ˜¯è§£å†»çš„
            _set_requires_grad(model.predictor_seg, True)
            _set_requires_grad(model.predictor_depth, True)
            if hasattr(model, 'predictor_normal'):
                _set_requires_grad(model.predictor_normal, True)

            # 2. è§£å†»æ‰€æœ‰ z_p ç§æœ‰åˆ†æ”¯
            _set_requires_grad(model.projector_p_seg, True)
            _set_requires_grad(model.projector_p_depth, True)
            _set_requires_grad(getattr(model, 'projector_p_normal', None), True)

            _set_requires_grad(model.proj_z_p_seg, True)
            _set_requires_grad(model.proj_z_p_depth, True)
            _set_requires_grad(getattr(model, 'proj_z_p_normal', None), True)

            _set_requires_grad(model.zp_seg_refiner, True)
            _set_requires_grad(model.zp_depth_refiner, True)
            _set_requires_grad(getattr(model, 'zp_normal_refiner', None), True)

            _set_requires_grad(model.decoder_zp_depth, True)
            _set_requires_grad(getattr(model, 'decoder_zp_normal', None), True)

            logging.info("Stage-2: unfrozen private (z_p) branches.")


def _get_lr(optimizer):
    for pg in optimizer.param_groups:
        return pg.get("lr", None)


def _set_lr(optimizer, lr):
    for pg in optimizer.param_groups:
        pg["lr"] = lr


def _build_scheduler(optimizer, train_cfg):
    """
    è‡ªåŠ¨æ„å»ºè°ƒåº¦å™¨ï¼šCosine æˆ– Step
    """
    base_lr = float(train_cfg.get("learning_rate", 1e-4))
    sched_cfg = train_cfg.get("lr_scheduler", {}) or {}
    sched_type = str(sched_cfg.get("type", "cosine")).lower()

    if sched_type == "cosine":
        warmup_epochs = int(sched_cfg.get("warmup_epochs", 3))
        min_lr_factor = float(sched_cfg.get("min_lr_factor", 0.1))
        total_epochs = int(train_cfg.get("epochs", 30))
        t_max = int(sched_cfg.get("T_max", max(1, total_epochs - warmup_epochs)))
        cosine = CosineAnnealingLR(
            optimizer,
            T_max=t_max,
            eta_min=base_lr * min_lr_factor
        )
        return {
            "type": "cosine",
            "warmup_epochs": warmup_epochs,
            "base_lr": base_lr,
            "cosine": cosine
        }

    # fallback: StepLR (LibMTL é»˜è®¤ä½¿ç”¨è¿™ä¸ª)
    step_size = int(sched_cfg.get("step_size", 100))
    gamma = float(sched_cfg.get("gamma", 0.5))
    step = StepLR(optimizer, step_size=step_size, gamma=gamma)
    return {
        "type": "step",
        "step": step
    }


# ----------------------------
# train loops
# ----------------------------
def train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, stage: int):
    model.train()
    total_train_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1} [Training]", leave=False)

    # è‡ªåŠ¨è®¡ç®—ç´¯ç§¯æ­¥æ•° (ä¿æŒåŸæœ‰é€»è¾‘)
    target_bs = 16
    physical_bs = train_loader.batch_size
    accumulation_steps = max(1, target_bs // physical_bs)

    optimizer.zero_grad(set_to_none=True)

    # [ä¿®æ”¹ 1] åˆ é™¤ scaler åˆå§‹åŒ–
    # scaler = GradScaler()  <-- åˆ é™¤

    for i, batch in enumerate(pbar):
        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}
        rgb = batch['rgb']

        # [ä¿®æ”¹ 2] åˆ é™¤ with autocast():ï¼Œç›´æ¥è¿è¡Œæ¨¡å‹
        # with autocast():  <-- åˆ é™¤è¿™ä¸€è¡Œï¼Œä¸‹é¢çš„ä»£ç å–æ¶ˆä¸€çº§ç¼©è¿›
        outputs = model(rgb, stage=stage)

        crit_out = criterion(outputs, batch)
        if isinstance(crit_out, (tuple, list)):
            total_loss, loss_dict = crit_out[0], crit_out[1]
        elif isinstance(crit_out, dict):
            loss_dict = crit_out
            total_loss = loss_dict.get('total_loss')
            if total_loss is None:
                raise ValueError("criterion returned dict but no 'total_loss' key found.")
        else:
            raise ValueError("criterion must return dict or (total_loss, dict).")

        loss_normalized = total_loss / accumulation_steps

        # [ä¿®æ”¹ 3] åˆ é™¤ scaler.scaleï¼Œç›´æ¥åå‘ä¼ æ’­
        # scaler.scale(loss_normalized).backward() <-- åˆ é™¤
        loss_normalized.backward()  # <-- æ”¹ä¸ºè¿™æ ·

        # æ¢¯åº¦æ›´æ–°
        if (i + 1) % accumulation_steps == 0:
            # [å¼ºçƒˆæ¨è] å³ä½¿æ˜¯ FP32ï¼Œä¿ç•™æ¢¯åº¦è£å‰ªä¹Ÿæ˜¯é˜²æ­¢è®­ç»ƒå´©æºƒçš„æœ€ä½³å®è·µ
            # ä¹‹å‰æåˆ°çš„ unscale_ ä¹Ÿä¸éœ€è¦äº†ï¼Œç›´æ¥ clip å³å¯
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)

            # [ä¿®æ”¹ 4] åˆ é™¤ scaler.step å’Œ scaler.updateï¼Œæ”¹å›æ ‡å‡† step
            # scaler.step(optimizer)  <-- åˆ é™¤
            # scaler.update()         <-- åˆ é™¤
            optimizer.step()  # <-- æ”¹ä¸ºè¿™æ ·

            optimizer.zero_grad(set_to_none=True)

        # è®°å½• Loss (åŠ ä¸ªé˜²æ­¢ NaN çš„åˆ¤æ–­ï¼Œè™½åœ¨ FP32 ä¸‹å¾ˆéš¾å‡ºç°)
        loss_val = total_loss.item()
        if not np.isfinite(loss_val):
            print(f"Warning: Non-finite loss {loss_val} at step {i}")

        total_train_loss += float(loss_val)
        pbar.set_postfix(loss=f"{loss_val:.4f}")

    # å¤„ç†å‰©ä½™æ¢¯åº¦ (å¦‚æœæœ‰)
    if len(train_loader) % accumulation_steps != 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

    avg_train_loss = total_train_loss / max(1, len(train_loader))
    logging.info(f"Epoch {epoch + 1} - Average Training Loss: {avg_train_loss:.4f}")
    return avg_train_loss

def calculate_improvement(base_metrics, current_metrics, data_type='nyuv2'):
    """ç›¸å¯¹æå‡ç‡è®¡ç®— (LibMTL å¯¹é½)"""
    improvement = 0
    count = 0
    # å®šä¹‰æŒ‡æ ‡æ–¹å‘: 1=è¶Šå¤§è¶Šå¥½, 0=è¶Šå°è¶Šå¥½
    metric_meta = {
        'seg_miou': 1, 'seg_pixel_acc': 1,
        'depth_abs_err': 0, 'depth_rel_err': 0,
        'normal_mean_angle': 0, 'normal_acc_30': 1,
        'normal_median_angle': 0, 'normal_acc_11': 1, 'normal_acc_22': 1
    }
    if 'gta5' in data_type:
        # Sim-to-Real: ä»…å…³æ³¨åˆ†å‰²ï¼Œå¿½ç•¥ Target åŸŸä¸Šæœªç»è®­ç»ƒçš„æ·±åº¦/æ³•çº¿
        valid_keys = {'seg_miou', 'seg_pixel_acc'}

    elif  data_type == 'cityscapes':
        # Cityscapes MTL: å…³æ³¨ åˆ†å‰² + æ·±åº¦ (æ³•çº¿ä¸å­˜åœ¨)
        valid_keys = {'seg_miou', 'seg_pixel_acc', 'depth_abs_err', 'depth_rel_err'}

    else:  # Default (e.g., 'nyuv2')
        # Indoor MTL: å…¨éƒ½è¦
        valid_keys = set(metric_meta.keys())
    for k, direction in metric_meta.items():
        if k not in valid_keys:
            continue
        if k in base_metrics and k in current_metrics:
            base = base_metrics[k]
            curr = current_metrics[k]
            if base == 0: continue

            # è¶Šå°è¶Šå¥½æ—¶ï¼š(Base - Curr) / Base
            # è¶Šå¤§è¶Šå¥½æ—¶ï¼š(Curr - Base) / Base
            if direction == 1:
                imp = (curr - base) / base
            else:
                imp = (base - curr) / base
            improvement += imp
            count += 1

    return improvement / max(1, count)


def train(model, train_loader, val_loader, optimizer, criterion, scheduler, config, device,
          checkpoint_dir='checkpoints'):
    # 1. æå– dataset_type (è½¬å°å†™ä»¥é˜²ä¸‡ä¸€)
    data_type = config['data'].get('type', 'nyuv2').lower()

    # 2. è¯»å–è®­ç»ƒé…ç½®
    train_cfg = config['training']
    stage1_epochs = int(train_cfg.get('stage1_epochs', 10))
    total_epochs = int(train_cfg.get('epochs', 30))
    base_lr = float(train_cfg.get("learning_rate", 1e-4))

    # 3. åˆå§‹åŒ–åŸºå‡†å˜é‡
    best_relative_score = -float('inf')
    baseline_metrics = None
    best_epoch = 0
    best_metrics_details = {}

    # æ„å»ºè°ƒåº¦å™¨
    sched = _build_scheduler(optimizer, train_cfg)
    logging.info(f"[LR Scheduler] {sched['type']}; base_lr={base_lr}")

    stage0_epochs = int(train_cfg.get('stage0_epochs', 0))
    for epoch in range(total_epochs):
        if epoch < stage0_epochs:
            stage = 0
        elif epoch < stage1_epochs:
            stage = 1
        else:
            stage = 2
        if epoch == 0 or epoch == stage0_epochs or epoch == stage1_epochs:
            _switch_stage_freeze(model, stage)
        target_ind_lambda = float(config['losses'].get('lambda_independence', 0.0))
        ind_warmup_epochs = int(train_cfg.get('ind_warmup_epochs', 0))

        current_ind_lambda = target_ind_lambda
        if stage < 2:
            # Stage 0/1 å¼ºåˆ¶ä¸º 0 (è™½ç„¶ Loss å†…éƒ¨ä¹Ÿæœ‰åˆ¤æ–­ï¼Œä½†è¿™é‡Œæ˜¾å¼æ§åˆ¶æ›´å®‰å…¨)
            current_ind_lambda = 0.0
        elif ind_warmup_epochs > 0:
            # Stage 2ï¼šå¼€å§‹ Warmup
            # å…³é”®ç‚¹ï¼šè¿›åº¦ = (å½“å‰Epoch - Stage2å¼€å§‹Epoch)
            progress = epoch - stage1_epochs

            # é™åˆ¶æ¯”ä¾‹åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´
            ratio = min(1.0, max(0.0, progress / float(ind_warmup_epochs)))
            current_ind_lambda = target_ind_lambda * ratio
        if hasattr(criterion, 'weights'):
            criterion.weights['lambda_independence'] = torch.tensor(current_ind_lambda, device=device)
        # ---- Warm-up (Cosine only) ----
        if sched["type"] == "cosine":
            warmup_epochs = sched["warmup_epochs"]
            if epoch < warmup_epochs:
                warmup_start = 0.1 * base_lr
                ratio = float(epoch + 1) / float(max(1, warmup_epochs))
                lr_now = warmup_start + (base_lr - warmup_start) * ratio
                _set_lr(optimizer, lr_now)
            else:
                if abs(_get_lr(optimizer) - base_lr) > 1e-12 and epoch == warmup_epochs:
                    _set_lr(optimizer, base_lr)

        cur_lr = _get_lr(optimizer)
        logging.info(f"\n----- Starting Epoch {epoch + 1}/{total_epochs} (Stage {stage}) | lr={cur_lr:.6f} -----")

        # --- Train ---
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, stage=stage)

        # --- Validate ---
        val_metrics = evaluate(model, val_loader, criterion, device, stage=stage,data_type=data_type)

        # --- Quick diagnose (Optional) ---
        if os.environ.get("QUICK_DIAG", "0") == "1" and (epoch == 0 or epoch == stage1_epochs):
            from engine.evaluator import quick_diagnose
            quick_diagnose(model, val_loader, device)

        # --- Step Scheduler ---
        if sched["type"] == "cosine":
            if epoch >= sched["warmup_epochs"]:
                sched["cosine"].step()
        else:
            sched["step"].step()

        # --- Best Model Selection Logic (LibMTL Aligned) ---
        if epoch < stage0_epochs:
            baseline_metrics = val_metrics
            is_best = True
            best_relative_score = 0.0
            logging.info("  -> Stage 0 won't service as Baseline for improvement calculation.")
        elif epoch == stage0_epochs:
            # Stage 1 Epoch 0 ä½œä¸ºåŸºå‡†çº¿
            baseline_metrics = val_metrics
            is_best = True
            best_relative_score = 0.0
            logging.info("  -> Stage 1 Epoch 0 set as Baseline for improvement calculation.")
        else:
            # è®¡ç®—ç›¸å¯¹äº Epoch 0 çš„æå‡
            score = calculate_improvement(baseline_metrics, val_metrics,data_type=data_type)
            is_best = (score > best_relative_score)

            if is_best:
                best_relative_score = score
                best_epoch = epoch + 1
                best_metrics_details = val_metrics.copy()  # ä¿å­˜æœ€ä½³æ—¶åˆ»çš„æŒ‡æ ‡å‰¯æœ¬
                logging.info(f"  -> ğŸ† New best model found! Avg Improvement vs Epoch 0: {score:.2%}")

                metrics_log = (
                    f"     [Tasks] Seg: mIoU={val_metrics.get('seg_miou', 0):.4f} Acc={val_metrics.get('seg_pixel_acc', 0):.4f} \n"
                )
                if "gta5" not in data_type:
                    metrics_log += (f"Depth: Abs={val_metrics.get('depth_abs_err', 0):.4f} Rel={val_metrics.get('depth_rel_err', 0):.4f} \n")
                # æ³•çº¿ (Normal)ï¼šåªåœ¨ NYUv2 ä¸‹æ‰“å° (ç¡¬é€»è¾‘)
                if 'nyuv2' in data_type:
                    metrics_log += (
                        f"\n     [Normal] Mean={val_metrics.get('normal_mean_angle', 0):.2f}Â° Med={val_metrics.get('normal_median_angle', 0):.2f}Â° | "
                        f"Acc: 11Â°={val_metrics.get('normal_acc_11', 0):.3f} 22Â°={val_metrics.get('normal_acc_22', 0):.3f} 30Â°={val_metrics.get('normal_acc_30', 0):.3f}"
                    )
                logging.info(metrics_log)

        save_checkpoint({
            'epoch': epoch + 1,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'best_score': best_relative_score,
        }, is_best, checkpoint_dir=checkpoint_dir)

    # =========================================================
    # [FINAL LOG] è®­ç»ƒç»“æŸæ—¶çš„è¯¦ç»†æ€»ç»“ (å·²ä¿®å¤æ•°æ®ç±»å‹åˆ¤æ–­)
    # =========================================================
    logging.info("\n" + "=" * 60)
    logging.info(f"ğŸ† Best Model Selection Summary (Epoch {best_epoch}):")
    logging.info(f"   Relative Improvement vs Epoch 0: {best_relative_score:.2%}")
    logging.info("-" * 60)
    logging.info("-- Best Epoch Downstream Task Metrics --")

    # 1. Segmentation (é€šç”¨)
    miou = best_metrics_details.get('seg_miou', 0.0)
    pix_acc = best_metrics_details.get('seg_pixel_acc', 0.0)
    logging.info(f"  - Segmentation: mIoU={miou:.4f}, Pixel Acc={pix_acc:.4f}")

    # 2. Depth (é€šç”¨)
    abs_err = best_metrics_details.get('depth_abs_err', 0.0)
    rel_err = best_metrics_details.get('depth_rel_err', 0.0)
    logging.info(f"  - Depth:        Abs Err={abs_err:.4f}, Rel Err={rel_err:.4f}")

    # 3. Normal (ä»… NYUv2 è¾“å‡º)
    if 'nyuv2' in data_type:
        mean_ang = best_metrics_details.get('normal_mean_angle', 0.0)
        med_ang = best_metrics_details.get('normal_median_angle', 0.0)
        acc_11 = best_metrics_details.get('normal_acc_11', 0.0)
        acc_22 = best_metrics_details.get('normal_acc_22', 0.0)
        acc_30 = best_metrics_details.get('normal_acc_30', 0.0)
        logging.info(f"  - Normal:       Mean Ang={mean_ang:.2f}Â°, Median Ang={med_ang:.2f}Â°")
        logging.info(f"                  Acc@11.25Â°={acc_11:.4f}, Acc@22.5Â°={acc_22:.4f}, Acc@30Â°={acc_30:.4f}")

    # 4. Scene (å·²åºŸå¼ƒï¼Œæ³¨é‡Šæ‰)
    # scene_acc = best_metrics_details.get('scene_acc', 1.0)
    # if scene_acc != 1.0:
    #     logging.info(f"  - Scene Classification (Acc): {scene_acc:.4f}")

    logging.info("=" * 60 + "\n")