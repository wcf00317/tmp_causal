import os
import torch
import fnmatch
import numpy as np
from torch.utils.data import Dataset


class RandomScaleCrop(object):
    def __init__(self, scale=[1.0, 1.2, 1.5]):
        self.scale = scale

    def __call__(self, img, label, depth, normal):
        # img: [C, H, W], label: [H, W], depth: [1, H, W], normal: [3, H, W]
        height, width = img.shape[-2:]
        sc = self.scale[random.randint(0, len(self.scale) - 1)]
        h, w = int(height / sc), int(width / sc)
        i = random.randint(0, height - h)
        j = random.randint(0, width - w)

        # Bilinear for continuous (img, normal), Nearest for discrete/depth (label, depth)
        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',
                             align_corners=True).squeeze(0)
        label_ = F.interpolate(label[None, None, i:i + h, j:j + w].float(), size=(height, width),
                               mode='nearest').squeeze(0).squeeze(0).long()
        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)
        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',
                                align_corners=True).squeeze(0)

        return img_, label_, depth_ / sc, normal_

class NYUv2Dataset(Dataset):
    def __init__(self, root_dir, mode='train', augmentation=False):
        """
        çº¯å‡€ç‰ˆ NYUv2 æ•°æ®é›†è¯»å–ç±»ã€‚
        ç›´æŽ¥è¯»å– LibMTL é¢„å¤„ç†å¥½çš„ .npy æ–‡ä»¶ã€‚
        å‡è®¾æ•°æ®å·²ç»æ˜¯ï¼š
        1. å°ºå¯¸å¯¹é½ (H, W ä¸€è‡´)
        2. æ•°å€¼æ­£ç¡® (Image [0,1], Label æ•´æ•°)
        3. ç»´åº¦é¡ºåºä¸º NumPy é»˜è®¤çš„ HWC
        ä¸åšä»»ä½•é¢å¤–çš„ Resizeã€Cropã€Flip æˆ– Normalizeã€‚
        """
        super().__init__()
        self.root = os.path.expanduser(root_dir)
        self.mode = mode
        self.augmentation = augmentation

        # LibMTL æ–‡ä»¶å¤¹ç»“æž„: root/train/image/*.npy
        sub_dir = 'train' if mode == 'train' else 'val'
        self.data_path = os.path.join(self.root, sub_dir)

        if not os.path.exists(self.data_path):
            raise ValueError(f"Data path not found: {self.data_path}")

        # èŽ·å–æ–‡ä»¶åˆ—è¡¨
        image_dir = os.path.join(self.data_path, 'image')
        # ä½¿ç”¨ fnmatch è¿‡æ»¤ .npy æ–‡ä»¶
        self.index_list = fnmatch.filter(os.listdir(image_dir), '*.npy')
        # æå–æ–‡ä»¶åä¸­çš„æ•°å­—å¹¶æŽ’åºï¼Œç¡®ä¿æ•°æ®å¯¹é½
        self.index_list = [int(x.replace('.npy', '')) for x in self.index_list]
        self.index_list.sort()

        self.num_samples = len(self.index_list)
        print(f"[{mode.upper()}] Found {self.num_samples} samples in {self.data_path}")

    def __getitem__(self, i):
        index = self.index_list[i]

        # ==================================================
        # 1. è¯»å– .npy æ–‡ä»¶ (NumPy é»˜è®¤ HWC æ ¼å¼)
        # ==================================================
        img_path = os.path.join(self.data_path, 'image', f'{index}.npy')
        label_path = os.path.join(self.data_path, 'label', f'{index}.npy')
        depth_path = os.path.join(self.data_path, 'depth', f'{index}.npy')
        normal_path = os.path.join(self.data_path, 'normal', f'{index}.npy')

        img_np = np.load(img_path)  # [H, W, 3]
        label_np = np.load(label_path)  # [H, W]
        depth_np = np.load(depth_path)  # [H, W] æˆ– [H, W, 1]
        normal_np = np.load(normal_path)  # [H, W, 3]

        # ==================================================
        # 2. ç»´åº¦å˜æ¢ (HWC -> CHW) & è½¬ Tensor
        # ==================================================
        # Image: [H, W, 3] -> [3, H, W]
        image = torch.from_numpy(np.moveaxis(img_np, -1, 0)).float()

        # Label: [H, W] -> [H, W] (è¯­ä¹‰åˆ†å‰²æ ‡ç­¾ä¸éœ€è¦ Channel ç»´)
        semantic = torch.from_numpy(label_np).long()

        # Depth: [H, W] æˆ– [H, W, 1] -> [1, H, W]
        if depth_np.ndim == 2:
            depth = torch.from_numpy(depth_np).float().unsqueeze(0)
        else:
            depth = torch.from_numpy(np.moveaxis(depth_np, -1, 0)).float()

        # Normal: [H, W, 3] -> [3, H, W]
        normal = torch.from_numpy(np.moveaxis(normal_np, -1, 0)).float()

        if self.augmentation:
            print("ðŸ”¥ Data Augmentation (RandomScaleCrop + Flip) is ENABLED via Config.")
            # 1. Random Scale Crop
            image, semantic, depth, normal = RandomScaleCrop()(image, semantic, depth, normal)
            # 2. Random Horizontal Flip
            if torch.rand(1) < 0.5:
                image = torch.flip(image, dims=[2])
                semantic = torch.flip(semantic, dims=[1])
                depth = torch.flip(depth, dims=[2])
                normal = torch.flip(normal, dims=[2])
                normal[0, :, :] = - normal[0, :, :]

        # ==================================================
        # 3. æž„é€ è¿”å›žå­—å…¸ (çº¯å‡€æ•°æ®)
        # ==================================================
        return {
            'rgb': image,  # [3, H, W], float32, æœªå½’ä¸€åŒ–
            'depth': depth,  # [1, H, W], float32
            'segmentation': semantic,  # [H, W], int64
            'normal': normal,  # [3, H, W], float32
            'scene_type': torch.tensor(0, dtype=torch.long),  # å ä½ç¬¦

            # Causal Model éœ€è¦çš„é¢å¤–é”®ï¼š
            'appearance_target': image,  # é‡æž„ç›®æ ‡ (ä¸Žè¾“å…¥ä¸€è‡´)
            'depth_meters': depth  # æ·±åº¦ (ä¸Ž depth ä¸€è‡´)
        }

    def __len__(self):
        return self.num_samples