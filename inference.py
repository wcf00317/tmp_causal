import os
import argparse
import yaml
import torch
import h5py
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from tqdm import tqdm
from torch.utils.data import DataLoader, random_split

# --- é¡¹ç›®æ¨¡å—å¯¼å…¥ ---
from models.causal_model import CausalMTLModel
from data_utils.nyuv2_dataset import NYUv2Dataset
# æˆ‘ä»¬åªå¯¼å…¥åŸºç¡€çš„ visualizer å‡½æ•°ï¼Œæ·±åº¦åˆ†æå‡½æ•°æˆ‘ä»¬åœ¨æœ¬æ–‡ä»¶é‡å†™
from engine.visualizer import _visualize_microscope, _visualize_mixer, denormalize_image
from utils.general_utils import set_seed


def parse_args():
    parser = argparse.ArgumentParser(description="Causal MTL Inference & Visualization Standalone Script")
    parser.add_argument('--config', type=str, required=True,
                        help="Path to the config file used for training")
    parser.add_argument('--checkpoint', type=str, required=True,
                        help="Path to the model checkpoint")
    parser.add_argument('--dataset_path', type=str, default=None, help="Override dataset path")
    parser.add_argument('--device', type=str, default='cuda', help="Device to use")
    parser.add_argument('--batch_size', type=int, default=1, help="Inference batch size")
    return parser.parse_args()


def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)


# --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘å°†æ·±åº¦è§£è€¦åˆ†æé€»è¾‘ç›´æ¥å†™åœ¨è¿™é‡Œï¼Œç¡®ä¿åŒ…å« Zs-only ---
# --- ã€ä¿®å¤ç‰ˆã€‘æœ¬åœ°å®šä¹‰çš„æ·±åº¦åˆ†æå‡½æ•° ---
def local_visualize_depth_task(model, batch, device, save_path):
    """
    æœ¬åœ°å®šä¹‰çš„æ·±åº¦åˆ†æå‡½æ•°ï¼Œå¼ºåˆ¶åŒ…å« Zs-Only å’Œ Zp-Only çš„å¯¹æ¯”ã€‚
    ã€ä¿®å¤ã€‘é€‚é… GatedSegDepthDecoder çš„åŒå‚æ•°æ¥å£ (main_feat, z_p_feat)ã€‚
    """
    model.eval()
    idx = 0
    rgb_tensor = batch['rgb'][idx].unsqueeze(0).to(device)  # [1, 3, H, W]

    with torch.no_grad():
        # 1. æ‰‹åŠ¨æ‹†è§£æ¨¡å‹å‰å‘è¿‡ç¨‹
        # ç¼–ç 
        features = model.encoder(rgb_tensor)  # List[Tensor]
        combined_feat = torch.cat(features, dim=1)  # [1, C*4, 14, 14]

        # æŠ•å½±
        f_proj = model.proj_f(combined_feat)

        # Z_s åˆ†æ”¯
        z_s_map = model.projector_s(combined_feat)
        zs_proj = model.proj_z_s(z_s_map)

        # Z_p åˆ†æ”¯ (Depth)
        z_p_depth_map = model.projector_p_depth(combined_feat)
        zp_depth_proj = model.proj_z_p_depth(z_p_depth_map)

        # --- æ„é€ è¾“å…¥ ---
        # GatedDecoder éœ€è¦ä¸¤ä¸ªè¾“å…¥: (main_feat, z_p_feat)
        # main_feat é€šå¸¸æ˜¯ f_proj å’Œ zs_proj çš„æ‹¼æ¥
        main_feat = torch.cat([f_proj, zs_proj], dim=1)

        # (A) Main Prediction: å®Œæ•´æ¨¡å‹ (z_p å‚ä¸é—¨æ§)
        pred_main = model.predictor_depth(main_feat, zp_depth_proj)

        # (B) Zs Only: å±è”½ z_p (ä¼ å…¥å…¨é›¶ä½œä¸ºé—¨æ§æ¡ä»¶)
        # è¿™å°†æµ‹è¯•ä»…é  f å’Œ zs èƒ½æ¢å¤å¤šå°‘ç»“æ„
        zeros_zp = torch.zeros_like(zp_depth_proj)
        pred_zs = model.predictor_depth(main_feat, zeros_zp)

        # (C) Zp Only: ä»…å¤–è§‚ (åº”è¯¥æ˜¯ä¸€å›¢ç³Ÿ/å™ªå£°)
        # ä½¿ç”¨ä¸“é—¨çš„è¾…åŠ©è§£ç å™¨ decoder_zp_depth (å®ƒåªæ¥å— z_p_map)
        pred_zp = model.decoder_zp_depth(z_p_depth_map)

    # 2. æ•°æ®è½¬æ¢
    input_rgb = denormalize_image(batch['rgb'][idx])
    gt_depth = batch['depth'][idx].squeeze().cpu().numpy()

    d_main = pred_main[0].squeeze().cpu().numpy()
    d_zs = pred_zs[0].squeeze().cpu().numpy()
    d_zp = pred_zp[0].squeeze().cpu().numpy()

    # è¯¯å·®å›¾
    error_map = np.abs(d_main - gt_depth)

    # 3. ç»˜å›¾ (1è¡Œ6åˆ—)
    fig, axes = plt.subplots(1, 6, figsize=(36, 6))
    fig.suptitle("Causal Depth Analysis: Can $z_s$ alone recover geometry?", fontsize=22)

    # ç»Ÿä¸€è‰²é˜¶
    vmin, vmax = np.percentile(gt_depth, [2, 98])

    # Col 1: RGB
    axes[0].imshow(input_rgb)
    axes[0].set_title("Input RGB", fontsize=16)

    # Col 2: GT
    axes[1].imshow(gt_depth, cmap='plasma', vmin=vmin, vmax=vmax)
    axes[1].set_title("Ground Truth", fontsize=16)

    # Col 3: Main
    axes[2].imshow(d_main, cmap='plasma', vmin=vmin, vmax=vmax)
    axes[2].set_title("Main Prediction\n($f + z_s + z_p$)", fontsize=16)

    # Col 4: Zs Only (é‡ç‚¹!)
    axes[3].imshow(d_zs, cmap='plasma', vmin=vmin, vmax=vmax)
    axes[3].set_title("Structure Only ($z_s$)\n(Should be clear)", fontsize=16)

    # Col 5: Zp Only
    axes[4].imshow(d_zp, cmap='plasma', vmin=vmin, vmax=vmax)
    axes[4].set_title("Appearance Only ($z_p$)\n(Should be noise)", fontsize=16)

    # Col 6: Error
    im_err = axes[5].imshow(error_map, cmap='hot')
    axes[5].set_title("Prediction Error", fontsize=16)

    for ax in axes.flat: ax.axis('off')

    fig.colorbar(im_err, ax=axes[5], fraction=0.046, pad=0.04)
    plt.tight_layout(rect=[0, 0.03, 1, 0.93])

    plt.savefig(save_path, bbox_inches='tight', dpi=150)
    plt.close(fig)
    print(f"  -> Saved Depth Analysis: {save_path}")

def main():
    args = parse_args()
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Using device: {device}")

    # 1. åŠ è½½é…ç½®
    print(f"ğŸ“‚ Loading config from: {args.config}")
    config = load_config(args.config)
    if args.dataset_path:
        config['data']['dataset_path'] = args.dataset_path

    # 2. å¤ç°æ•°æ®åˆ’åˆ†
    seed = config['training']['seed']
    set_seed(seed)
    print(f"ğŸŒ± Re-seeding with {seed} ...")

    print("ğŸ“š Initializing Dataset...")
    with h5py.File(config['data']['dataset_path'], 'r') as db:
        scene_type_refs = db['sceneTypes']
        scene_types_list = []
        for i in range(scene_type_refs.shape[1]):
            ref = scene_type_refs[0, i]
            scene_str = "".join(chr(c[0]) for c in db[ref])
            scene_types_list.append(scene_str)

    full_dataset = NYUv2Dataset(
        mat_file_path=config['data']['dataset_path'],
        img_size=tuple(config['data']['img_size']),
        scene_types_list=scene_types_list
    )

    g = torch.Generator()
    g.manual_seed(seed)
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    _, val_dataset = random_split(full_dataset, [train_size, val_size], generator=g)

    print(f"âœ… Data split reproduced. Val size: {len(val_dataset)}")

    # 3. Loader
    val_loader = DataLoader(
        val_dataset,
        batch_size=max(2, args.batch_size),
        shuffle=True,  # éšæœºæ‰“ä¹±ä»¥ç”Ÿæˆå¤šæ ·åŒ–å›¾ç‰‡
        num_workers=2,
        pin_memory=True
    )

    # 4. Model
    print("âš™ï¸ Building Model...")
    model = CausalMTLModel(
        model_config=config['model'],
        data_config=config['data']
    ).to(device)

    # 5. Checkpoint
    print(f"ğŸ“¥ Loading checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    try:
        model.load_state_dict(checkpoint['state_dict'])
        print("âœ… Weights loaded (Strict).")
    except RuntimeError as e:
        print(f"âš ï¸ Strict load failed, trying non-strict... {e}")
        model.load_state_dict(checkpoint['state_dict'], strict=False)
        print("âœ… Weights loaded (Non-Strict).")

    model.eval()

    # 6. Output Directories
    ckpt_dir = os.path.dirname(os.path.abspath(args.checkpoint))
    run_root = os.path.dirname(ckpt_dir)
    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    output_root = os.path.join(run_root, f"inference_results_{timestamp}")

    dir_microscope = os.path.join(output_root, "1_Causal_Microscope")
    dir_mixer = os.path.join(output_root, "2_Causal_Mixer_Swap")
    dir_depth = os.path.join(output_root, "3_Depth_Decoupling_Analysis")

    os.makedirs(dir_microscope, exist_ok=True)
    os.makedirs(dir_mixer, exist_ok=True)
    os.makedirs(dir_depth, exist_ok=True)
    print(f"ğŸ“‚ Saving results to: {output_root}")

    # 7. Loop
    print("ğŸ¬ Starting Inference Loop...")
    scene_class_map = full_dataset.scene_classes

    for batch_idx, batch in enumerate(tqdm(val_loader, desc="Generating")):
        # å…¨å±€ç¦æ­¢æ¢¯åº¦è®¡ç®—ï¼Œé˜²æ­¢ OOM å’ŒæŠ¥é”™
        with torch.no_grad():
            if batch['rgb'].shape[0] < 2:
                continue

            fname = f"sample_{batch_idx:04d}.png"

            try:
                # Task 1: Microscope (Basic Recon)
                save_path_micro = os.path.join(dir_microscope, fname)
                _visualize_microscope(model, batch, device, save_path_micro, scene_class_map)

                # Task 2: Mixer (Swap)
                batch_a = {k: v[0:1] for k, v in batch.items()}
                batch_b = {k: v[1:2] for k, v in batch.items()}
                save_path_mixer = os.path.join(dir_mixer, fname)
                _visualize_mixer(model, batch_a, batch_b, device, save_path_mixer, scene_class_map)

                # Task 3: Depth Analysis (è°ƒç”¨æœ¬åœ°å®šä¹‰çš„å‡½æ•°!)
                save_path_depth = os.path.join(dir_depth, fname)
                local_visualize_depth_task(model, batch, device, save_path_depth)

            except Exception as e:
                print(f"âŒ Batch {batch_idx} error: {e}")
                continue

    if hasattr(full_dataset, "close"):
        full_dataset.close()
    print(f"\nâœ¨ All Done! Results saved in: {output_root}")


if __name__ == "__main__":
    main()