import torch
import torch.nn as nn
from .building_blocks import ViTEncoder, MLP
# å¤ç”¨ä½ åœ¨ causal_model.py ä¸­å®šä¹‰çš„æ™®é€šè§£ç å™¨
from .causal_model import SegDepthDecoder
import torch.nn.functional as F


class SingleTaskModel(nn.Module):
    """
    Single-Task Learning (STL) Baseline.

    ã€ä¿®æ­£ç‰ˆã€‘
    å¢åŠ äº† Dummy Predictorsï¼Œä»¥å…¼å®¹ evaluator.py çš„å±æ€§è¯»å–éœ€æ±‚ã€‚
    æ— è®ºå½“å‰æ¿€æ´»å“ªä¸ªä»»åŠ¡ï¼Œéƒ½ä¼šæä¾› num_seg_classes å’Œ num_scene_classes ä¿¡æ¯ã€‚
    """

    def __init__(self, model_config, data_config):
        super().__init__()
        self.config = model_config
        self.active_task = model_config.get('active_task', 'seg')
        self.img_size = tuple(data_config['img_size'])

        print(f"ğŸ”’ Initializing Single-Task Model for: [{self.active_task.upper()}]")

        # 1. Backbone
        self.encoder = ViTEncoder(
            name=model_config['encoder_name'],
            pretrained=model_config['pretrained'],
            img_size=self.img_size[0]
        )
        encoder_feature_dim = self.encoder.feature_dim

        # 2. Projection
        combined_feature_dim = encoder_feature_dim * 4
        self.shared_dim = 512
        self.shared_proj = nn.Conv2d(combined_feature_dim, self.shared_dim, kernel_size=1)

        # 3. Task Configs
        self.num_seg_classes = 40
        self.num_scene_classes = model_config.get('num_scene_classes', 27)

        # 4. Initialize Heads & Dummy Attributes
        # -------------------------------------------------------------------------
        # å…³é”®ä¿®æ”¹ï¼šæ— è®ºæ¿€æ´»å“ªä¸ªä»»åŠ¡ï¼Œéƒ½ç¡®ä¿ self.predictor_xxx å­˜åœ¨ï¼Œä¸”æœ‰å¿…è¦çš„å±æ€§
        # -------------------------------------------------------------------------

        # --- Segmentation ---
        if self.active_task == 'seg':
            self.seg_head = SegDepthDecoder(input_channels=self.shared_dim, output_channels=self.num_seg_classes)
            self.predictor_seg = self.seg_head
        else:
            self.seg_head = None
            # Dummy Object with .output_channels
            self.predictor_seg = nn.Module()
            self.predictor_seg.output_channels = self.num_seg_classes

        # --- Depth ---
        if self.active_task == 'depth':
            self.depth_head = SegDepthDecoder(input_channels=self.shared_dim, output_channels=1)
            self.predictor_depth = self.depth_head
        else:
            self.depth_head = None
            self.predictor_depth = nn.Module()  # depth head usually has no specific attr accessed by evaluator

        # --- Scene ---
        if self.active_task == 'scene':
            self.scene_mlp = MLP(self.shared_dim, self.num_scene_classes, hidden_dim=256)
            self.predictor_scene = self.scene_mlp
        else:
            self.scene_mlp = None
            # Dummy Object with .out_features
            self.predictor_scene = nn.Module()
            self.predictor_scene.out_features = self.num_scene_classes

        # 5. Trainer Compatibility (Placeholders)
        self.projector_p_seg = None
        self.projector_p_depth = None
        self.proj_z_p_seg = None
        self.proj_z_p_depth = None
        self.zp_seg_refiner = None
        self.zp_depth_refiner = None
        self.decoder_zp_depth = None

    def forward(self, x, stage=None):
        B, _, H, W = x.shape

        # 1. Encoder
        features = self.encoder(x)

        # 2. Aggregation
        combined_feat = torch.cat(features, dim=1)
        shared_feat = self.shared_proj(combined_feat)

        outputs = {}

        # 3. Active Task Forward (others return Zeros)
        # æ³¨æ„ï¼ševaluator ä¼šæŠŠè¿™äº›å…¨é›¶çš„è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾åšè®¡ç®—ï¼Œå¾—åˆ°å¾ˆå·®çš„åˆ†æ•°ï¼ˆè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„ï¼‰

        # Seg
        if self.active_task == 'seg':
            outputs['pred_seg'] = self.seg_head(shared_feat)
        else:
            outputs['pred_seg'] = torch.zeros(B, self.num_seg_classes, H, W, device=x.device)

        # Depth
        if self.active_task == 'depth':
            outputs['pred_depth'] = self.depth_head(shared_feat)
        else:
            outputs['pred_depth'] = torch.zeros(B, 1, H, W, device=x.device)

        # Scene
        if self.active_task == 'scene':
            h = torch.nn.functional.adaptive_avg_pool2d(shared_feat, (1, 1)).flatten(1)
            outputs['pred_scene'] = self.scene_mlp(h)
        else:
            outputs['pred_scene'] = torch.zeros(B, self.num_scene_classes, device=x.device)

        return outputs

class RawMTLModel(nn.Module):
    """
    Standard Hard Parameter Sharing Multi-Task Learning (Raw MTL).

    ã€å·¥ç¨‹é€‚é…ç‰ˆã€‘
    åŒ…å«ç‰¹å®šçš„å±æ€§åˆ«åå’Œå ä½ç¬¦ï¼Œä»¥å…¼å®¹ Causal MTL é¡¹ç›®ç°æœ‰çš„ trainer.py å’Œ evaluator.pyï¼Œ
    æ— éœ€ä¿®æ”¹å…¬å…±è®­ç»ƒä»£ç ã€‚
    """

    def __init__(self, model_config, data_config):
        super().__init__()
        self.config = model_config

        # 1. Shared Backbone (Same as Ours)
        self.encoder = ViTEncoder(
            name=model_config['encoder_name'],
            pretrained=model_config['pretrained'],
            img_size=data_config['img_size'][0]
        )
        encoder_feature_dim = self.encoder.feature_dim

        # 2. Shared Projection (Bottleneck)
        combined_feature_dim = encoder_feature_dim * 4
        self.shared_dim = 512
        self.shared_proj = nn.Conv2d(combined_feature_dim, self.shared_dim, kernel_size=1)

        # 3. Task Heads

        # --- Scene Head ---
        self.num_scene_classes = model_config['num_scene_classes']
        # æ³¨æ„ï¼šä¸ºäº†å…¼å®¹ evaluator è¯»å– .out_featuresï¼Œæˆ‘ä»¬æŠŠ MLP ç‹¬ç«‹å‡ºæ¥
        self.scene_mlp = MLP(self.shared_dim, self.num_scene_classes, hidden_dim=256)

        # --- Seg Head ---
        self.num_seg_classes = 40
        self.seg_head = SegDepthDecoder(input_channels=self.shared_dim, output_channels=self.num_seg_classes)

        # --- Depth Head ---
        self.depth_head = SegDepthDecoder(input_channels=self.shared_dim, output_channels=1)

        # =========================================================
        #   å…¼å®¹æ€§é€‚é…å±‚ (Compatibility Layer) - å…³é”®ä¿®æ”¹
        # =========================================================

        # 1. [For Evaluator] æä¾›åˆ«åï¼Œå› ä¸º evaluator.py è®¿é—®çš„æ˜¯ predictor_xxx
        self.predictor_seg = self.seg_head
        self.predictor_depth = self.depth_head
        self.predictor_scene = self.scene_mlp  # evaluator è¯»å– .out_features

        # 2. [For Trainer] æä¾›å ä½ç¬¦ï¼Œæ¬ºéª— _switch_stage_freeze
        # trainer.py ä¼šå°è¯•è®¿é—®è¿™äº›å±æ€§å¹¶è®¾ç½®æ¢¯åº¦ã€‚è®¾ä¸º None å¯å®‰å…¨è·³è¿‡ã€‚
        self.projector_p_seg = None
        self.projector_p_depth = None
        self.proj_z_p_seg = None
        self.proj_z_p_depth = None
        self.zp_seg_refiner = None
        self.zp_depth_refiner = None
        self.decoder_zp_depth = None

        # 3. [For GradNorm] é¢„ç•™å…±äº«å±‚æ¥å£
        self.layer_to_norm = self.shared_proj

    def forward(self, x, stage=None):
        # stage å‚æ•°è¢«ä¿ç•™ä»¥å…¼å®¹ trainer æ¥å£è°ƒç”¨ï¼Œä½†æ­¤å¤„å¿½ç•¥

        # 1. Encoder
        features = self.encoder(x)

        # 2. Aggregation
        combined_feat = torch.cat(features, dim=1)
        shared_feat = self.shared_proj(combined_feat)

        # 3. Task Predictions
        pred_seg = self.seg_head(shared_feat)
        pred_depth = self.depth_head(shared_feat)

        # Scene: Pool -> Flatten -> MLP
        h = F.adaptive_avg_pool2d(shared_feat, (1, 1)).flatten(1)
        pred_scene = self.scene_mlp(h)

        return {
            'pred_seg': pred_seg,
            'pred_depth': pred_depth,
            'pred_scene': pred_scene,
            # 'z_s', 'z_p' ç­‰ä¸éœ€è¦è¿”å›ï¼Œå› ä¸ºä½¿ç”¨çš„æ˜¯ MTLLoss è€Œé CompositeLoss
        }

    def get_last_shared_layer(self):
        return self.layer_to_norm